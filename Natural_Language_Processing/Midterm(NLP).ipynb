{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-requisites:\n",
        "1. brain in cool mode\n",
        "2. patience\n",
        "3. chatgpt(if u didnt get the things)\n",
        "4. That's it"
      ],
      "metadata": {
        "id": "EHjZKCppPO5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**File Handling**"
      ],
      "metadata": {
        "id": "dcbYS9oAAGgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stuff = open(\"/content/dd.txt\").read()\n",
        "print(stuff)\n",
        "print(\"Total Words are :- \",len(stuff.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AskKW4PE9q4U",
        "outputId": "2cd9e3ce-1cb3-4b07-8a5f-aea8162b3807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What the hell bro! \n",
            "Total Words are :-  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORD TOKENIZER(using nltk)**"
      ],
      "metadata": {
        "id": "0y1bEYqOBT18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt_tab\")\n",
        "text = \"Jai Matha Di, Jai Radha Rani\"\n",
        "tk = word_tokenize(text)\n",
        "print(tk)\n",
        "print(len(tk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrUaPLKl-Xao",
        "outputId": "f748d34b-db8c-4542-f9aa-126b3ff9d7c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jai', 'Matha', 'Di', ',', 'Jai', 'Radha', 'Rani']\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt_tab\")\n",
        "text = \"Hello What's Up!!\"\n",
        "\n",
        "pt = word_tokenize(text)\n",
        "print(pt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IePiXUxp_lu1",
        "outputId": "f054db57-5576-49f4-c10a-4022dbf73c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'What', \"'s\", 'Up', '!', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORD TOKENIZER(using spacy)**\n"
      ],
      "metadata": {
        "id": "J3wIhXwrBhQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Damn it man!, Whats wrong with this elective!\"\n",
        "docnlp = nlp(text)\n",
        "print(docnlp)\n",
        "tokens = [token.text for token in docnlp]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2nnCGJrBl_E",
        "outputId": "e867a3a8-7fa3-4b10-e563-710eb3dd7b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Damn it man!, Whats wrong with this elective!\n",
            "['Damn', 'it', 'man', '!', ',', 'What', 's', 'wrong', 'with', 'this', 'elective', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lower** **casing**"
      ],
      "metadata": {
        "id": "6zdjdEvgDwC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Teacher is a very Sacred Word! Not every Faculty should be cAlled as a teacher!\"\n",
        "print(text)\n",
        "text = text.lower()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZHrTcZvCZ-U",
        "outputId": "77fc56e8-abb2-431a-afdc-da972afa2d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher is a very Sacred Word! Not every Faculty should be cAlled as a teacher!\n",
            "teacher is a very sacred word! not every faculty should be called as a teacher!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "gCPa6oC0D-y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tkword = word_tokenize(text)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV3gSjL6DNzq",
        "outputId": "b09985fd-5cef-40de-da31-e2481600aa5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "teacher is a very sacred word! not every faculty should be called as a teacher!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lematization**"
      ],
      "metadata": {
        "id": "oGVs4htMEcY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmat = [token.lemma_ for token in nlp(text)]\n",
        "print(lemmat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9geLC-s7EPHy",
        "outputId": "80fa0cb9-0e8e-40e3-fcb2-9b185698b595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['teacher', 'be', 'a', 'very', 'sacred', 'word', '!', 'not', 'every', 'faculty', 'should', 'be', 'call', 'as', 'a', 'teacher', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "kUDZc29lFBbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"Teaching is a very sacred post in our culture, but some of the modern Teaches behave shamelessly and try showing their attitude to the students with is useless and disregardful\"\n",
        "tokenizeda = word_tokenize(a)\n",
        "from nltk.stem import PorterStemmer\n",
        "rooter = PorterStemmer()\n",
        "root = [rooter.stem(word) for word in tokenizeda]\n",
        "print(root)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ-lnZ0QEzBD",
        "outputId": "d2abba76-2f82-4949-e2b5-6bd8ff054db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['teach', 'is', 'a', 'veri', 'sacr', 'post', 'in', 'our', 'cultur', ',', 'but', 'some', 'of', 'the', 'modern', 'teach', 'behav', 'shamelessli', 'and', 'tri', 'show', 'their', 'attitud', 'to', 'the', 'student', 'with', 'is', 'useless', 'and', 'disregard']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SpellChecker**"
      ],
      "metadata": {
        "id": "co-utT5eIc5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "textdemo = \"Insted of Techign what the method does the faculties simply make the studenst mugup things, such a shameless thing! \"\n",
        "textdemo = textdemo.lower()\n",
        "tokenfirst = word_tokenize(textdemo)\n",
        "spellcrtr = SpellChecker()\n",
        "wrong = [spellcrtr.correction(word) or word for word in tokenfirst]\n",
        "print(wrong)"
      ],
      "metadata": {
        "id": "wuFhzXv8F6sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca69a297-1219-45be-e1eb-4a270ea400fc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)\n",
            "['instead', 'of', 'teaching', 'what', 'the', 'method', 'does', 'the', 'faculties', 'simply', 'make', 'the', 'student', 'mug', 'things', ',', 'such', 'a', 'shameless', 'thing', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Vector Usage (Similarity, Analogy, etc.)**"
      ],
      "metadata": {
        "id": "7dxgm2k8K-ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVpohjktK7RD",
        "outputId": "20a8d161-b714-4c00-fb69-3e40fccf4cff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "mymodel = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m9izvdPLDsx",
        "outputId": "a9948536-d4b8-4801-f5d8-0462f2db92b9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Similarity\n",
        "print(mymodel.most_similar(\"mouse\"))\n",
        "print(mymodel.most_similar(\"lion\"))\n",
        "print(mymodel.most_similar(\"mad\"))\n",
        "print(mymodel.most_similar(\"scientist\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-R1fKCSLZHU",
        "outputId": "a4fa686f-18ed-47a9-b18e-580f49233774"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('monkey', 0.7965501546859741), ('bugs', 0.7805658578872681), ('cat', 0.7731667160987854), ('rabbit', 0.7622702717781067), ('worm', 0.7504912614822388), ('clone', 0.7307788729667664), ('robot', 0.7268993854522705), ('spider', 0.7199547290802002), ('bug', 0.7104865312576294), ('frog', 0.702705979347229)]\n",
            "[('dragon', 0.7917535901069641), ('elephant', 0.7359082102775574), ('beast', 0.7229052186012268), ('golden', 0.7137690782546997), ('cat', 0.7007724642753601), ('unicorn', 0.6967454552650452), ('bear', 0.6925660371780396), ('rabbit', 0.6621813178062439), ('spider', 0.6460384130477905), ('mermaid', 0.645137369632721)]\n",
            "[('cow', 0.7608286738395691), ('scare', 0.731239378452301), ('dog', 0.7244355082511902), ('cat', 0.7164114117622375), ('crazy', 0.7075807452201843), ('pig', 0.6972351670265198), ('killer', 0.6965683698654175), ('rabbit', 0.6916881203651428), ('bug', 0.6887489557266235), ('witch', 0.6708003282546997)]\n",
            "[('researcher', 0.858868420124054), ('physicist', 0.8501760363578796), ('expert', 0.8269404768943787), ('biologist', 0.8069736361503601), ('professor', 0.8030245900154114), ('scholar', 0.7544975876808167), ('psychologist', 0.7536154985427856), ('geologist', 0.7528449892997742), ('dr.', 0.7525599598884583), ('sociologist', 0.7416869401931763)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Analogy\n",
        "print(mymodel.similarity(\"boy\",\"bike\"))\n",
        "print(mymodel.similarity(\"hello\",\"you\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK9flbowLuNe",
        "outputId": "2409c055-0ded-4c58-8e2b-67fa111c64fa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3690389\n",
            "0.5642295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Language Detection**"
      ],
      "metadata": {
        "id": "kM6BFReuNWV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBoH01EGMhye",
        "outputId": "7cfac8f2-1537-407f-ea31-ece73b148899"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=af4b14ae3df951cac4621fbb1118db0dce976a9a028bee4f7543d4ae17644973\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect\n",
        "demotext = \"Bonjour\"\n",
        "print(detect(demotext))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q09CWdoNcVc",
        "outputId": "48eacedc-f54a-402b-c5f7-4c7b47c8ecb8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hr\n"
          ]
        }
      ]
    }
  ]
}